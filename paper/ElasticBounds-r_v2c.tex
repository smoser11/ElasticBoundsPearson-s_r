% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{fancyvrb}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}


\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\VerbatimFootnotes % allow verbatim text in footnotes
\hypersetup{
  pdftitle={Elastic Bounds of Pearson's r: Theoretical and Empirical Insights for Ordinal Variables},
  pdfauthor={Cees van der Eijk; Scott Moser},
  colorlinks=true,
  linkcolor={black},
  filecolor={Maroon},
  citecolor={RoyalBlue},
  urlcolor={BrickRed},
  pdfcreator={LaTeX via pandoc}}


\title{Elastic Bounds of Pearson's \(r\): Theoretical and Empirical
Insights for Ordinal Variables}
\author{Cees van der Eijk \and Scott Moser}
\date{}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

This is a test citation: Whitt (\citeproc{ref-whit76-bivariate}{1976}).

Another test: (\citeproc{ref-whit76-bivariate}{Whitt, 1976}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Algorithm Name}

\NormalTok{Step 1: Do this  }
\NormalTok{Step 2: Do that  }
\end{Highlighting}
\end{Shaded}

\textbf{Algorithm:} My Algorithm

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Do something\\
\item
  Do something else\\
\end{enumerate}

Just a sample algorithmn

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Write here the result}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Write here the input}
\Output{Write here the output}
\BlankLine
\While{While condition}{
    instructions\;
    \eIf{condition}{
        instructions1\;
        instructions2\;
    }{
        instructions3\;
    }
}
\caption{While loop with If/Else condition}
\end{algorithm}

\newpage

\begin{abstract}
The use of product-moment correlations to assess relationships between ordered-categorical variables is widespread and generally tolerated in spite of the non-metric character of such variables. Potential problems of this practice have been identified in the literature. This article focusses on one of these, namely that correlation coefficients are poorly comparable across pairs of ordered-categorical variables because their ranges are constrained in non-uniform ways. This not only affects the validity of correlation coefficients for descriptive purposes, but also as bases for subsequent analyses, as in, e.g., principal component analysis, factor analysis, and structural equation modelling. This article focusses on this problem of ‘constrained’ correlations. It explains the problem and explores conditions under which it is most severe. It provides illustrations based on both fabricated and actual empirical data, the latter from a mass-survey that ubiquitously contains many ordered-categorical variables that are commonly analysed with product-moment correlations. The article is accompanied by a software tool in R for analysing the attainable upper- and lower bounds of correlations in actual data. 

Pearson’s correlation coefficient $r$, when applied to ordinal data, is subject to elasticity due to constraints from marginal distributions. This paper develops theoretical bounds for $r$, based on Boole–Fréchet–Hoeffding inequalities and rearrangement inequalities, and derives analytic expressions for their limits. We investigate when symmetry in the bounds holds or breaks, and explore practical implications for correlation-based methods like PCA and SEM. An accompanying R package allows users to compute bounds and perform hypothesis tests given fixed marginals.
\end{abstract}

\newpage

\section{Preamble}\label{preamble}

\begin{itemize}
\item
  The widespread use of Pearson's \(r\) with ordinal data.\\
\item
  The problem of elasticity: how max/min bounds of \(r\) vary with
  marginals.\\
\item
  Contributions:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Theory of optimal bounds\\
  \item
    Symmetry-breaking conditions\\
  \item
    Empirical consequences \(\leadsto\) hypo testing and significance
  \item
    Supporting tools
  \end{enumerate}
\end{itemize}

The `Point':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Alert} applied researachers of `elastic bounds'

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Problems of ignoring the above \(\leadsto\) errors in interpretation
    (e.g.~Cohen and Cohen)
  \end{enumerate}
\item
  \emph{Provide} tools to assess if problems exist in own data/ research
\item
  \emph{Clarify} that traditional hypothesis testing (e.g.~t-test of
  \(H_0: r= 0\)) are incorrect and provide correct hypo testing
  (permutation/ randomization tests)
\end{enumerate}

\section{Introduction}\label{introduction}

In this article we elaborate why correlations involving
ordered-categorical variables are generally `constrained', which means
that their attainable maximum and minimum values are not +1 and -1, as
is often thought, but between values that have to be calculated for each
pair of variables. We assess the magnitude of these constraints in
fictitious as well as empirical data and analyse under which conditions
they are most severe. The article is accompanied by a software tool in R
that allows applied researchers to easily assess the extent of the
problem in their own data. The use of product-moment correlations (also
known aa Pearson correlations) to assess relationships between
ordered-categorical variables --henceforth referred to as ordcats-- is
widespread and widely condoned in empirical studies, in spite of such
variables not being metric while the product-moment correlation assumes
that they are. We do not focus on this problem from an obsessive desire
for methodological purity, were it only because many statistical
procedures are quite robust for violations of their assumptions {[}fn:
examples?{]} . Instead, we draw attention to a problematic consequence
of this violation of assumptions that, as far as we know, has been
rarely recognised in the applied literature although it is well-known in
the theoretical statistical literature. This is the problem that
correlation coefficients involving ordcats are constrained in their
range which may make them mutually incomparable and may lead to further
problems downstream the analytical process. This article proceeds as
follows. We start by briefly elaborating the character of ordinal
categorical variables (ordcats), why they are generally not regarded as
being metric, and why, in spite of that, they are nevertheless very
frequently used in statistical procedures that assume metric level data.
We then proceed to show why correlations based on ordcats tend to be
constrained, which implies that their range is smaller than from -1 to
+1, and why this is problematic in applied research. This raises the
question how to determine the actual range of correlations between
ordcats, which we address in the subsequent section. In section 5 we
assess the magnitude of these constraining effects on 7503 correlations
between 123 ordcats from a mass survey that typically gives rise ordered
categorical variables (ordcats), the British Election Study. We use the
same data to explore conditions that influence the magnitude of the
constraining effect. {[}Other sections in between?{]} In the concluding
section we discuss implications of our findings, suggestions for dealing
with this problem for applied researchers, and directions for further
research.

\subsection{On ordinal-categorical variables (ordcats), their level of
measurement and their common use in quantitative
analyses}\label{on-ordinal-categorical-variables-ordcats-their-level-of-measurement-and-their-common-use-in-quantitative-analyses}

Ordcats are ordinal variables with a relatively small number of
categories (or ranks) compared to the number of cases, thus resulting in
partial orderings of cases. Although they can be found in all field of
quantitative inquiry they are ubiquitous in data from standardised
social science surveys. Likert items and rating scales {[}describe in fn
Likert items and rating scales, add that there are yet other formats
such as semantic-differential scales that also yield ordcats{]} are
among the most favoured ways to elicit responses in such surveys. These
survey-question formats have most often between 3 and 11 ordinal
categories {[}fn (or not?) about ANES `feeling thermometer' which has
101 categories but in view of the clustering of responses around only
some values can perhaps better be regarded as a 13-point ordcat{]}. The
numerical values used to distinguish the categories of ordcats are most
often consecutive integers, starting with either 0 or 1, and in applied
survey data analyses it is common to interpret these values as scores. A
frequently issued objection against this so-called equal-distance
scoring practice {[}fn: add references{]} is that it implies that the
difference in meaning between, e.g., categories 1 and 2 is equal to that
between categories 3 and 4, etc., without clear justification. In other
words, it implies an interval-level interpretation by fiat. {[}add fn:
In principle it is quite possible to assign values to survey-based
ordcats that have a clear empirical justification, for example by
calibrating against psychophysical measurements (cf., Lodge; Tillie;
others), or against the results of psychometric/linguistic analysis of
verbal response labels (cf Sönning 2024). However, such procedures are
rarely integrated in applied survey research because of their high
costs.{]} The consequence of this objection is that ordcats should be
considered ordinal variables and that they should not be analysed with
statistical procedures requiring metric data. In spite of this objection
we find that ordcats are very commonly analysed with product-moment
correlations or with procedures that build upon such correlations. We
find this clearly exhibited in published articles in high-ranking
academic journals {[}fn: explain how we did this{]} and in highly
regarded textbooks where examples are given of the use of correlations
between ordcats as input for procedures such as PCA, FA or SEM {[}fn:
examples: Kline; Byrne; xxx{]}. This practice is so established that it
is occasionally rationalised by referring to ordcats as being
`quasi-interval level' measures {[}fn: reference{]}. One can, however,
also find some justifications in the literature for this practice, which
emphasises the robustness of correlation coefficients for violations of
variables involved . Labovitz (1967, 1970) in particular has compared
correlations between ordcats using the equal-scoring distance with a
monotonic random scoring (a procedure that only assumes ordinality
between categories). He claims that the differences are negligible,
which suggests that eve n if the theoretical objection against unfounded
interval level interpretations of ordcats would be justified, that
objection would nevertheless have such minor practical consequences as
to make to irrelevant in most situations. Not surprisingly, Labovitz'
work has, in turn be criticised itself on various grounds, most
particularly of (inadvertently) mainly covering situations where this
robustness holds, and not very well other situations (cf O'Brien 1979;
xxx). At this place we will not focus on discussions about the
equal-distance scoring practice. Our main reason for referring to it is
to demonstrate the existence of lively debates in the applied literature
about potential issues when using ordcats in procedures requiring metric
data. The existence of these debates makes it remarkable that another
important issue has --to the best of our knowledge-- remained mainly
overlooked in the applied literature while it has been well-known in the
theoretical statistical literature. This is the problem that
correlations between ordcats are generally `constrained', which means
that they do not range between -1 and +1, but between minima and maxima
that have to be calculated separately for each pair. The nature of this
issue is the subject of the next section.

\subsection{Ordinal categorical variables (ordcats) and `constrained'
correlations}\label{ordinal-categorical-variables-ordcats-and-constrained-correlations}

\begin{itemize}
\tightlist
\item
  Simple example of 4x4 crosstab
\item
  Conclusion: for every given distribution of a variable, there are only
  two distributions of the other variable which do not constrain r: the
  same, and the inverse. Every other distribution does constrain r
\item
  In case of constrainment (`shrinkage'?), the relationship becomes
  partially nonlinear, by necessity (does this means that the problem of
  constrainment does not exist for eta?) Is there a measure for
  nonlinearity? (polynomial multiple correlation?)
\end{itemize}

Determining the minimum and maximum attainable values of r for given
marginal distributions

\begin{itemize}
\tightlist
\item
  Explain different strategies which will all yield the same answers:
  separate sorting and pairing / Frechet-Hoeffding bounds/ etc
\item
  Proofs in appendix?
\end{itemize}

How severely are correlations constrained -- or: how long is a piece of
string?

\begin{itemize}
\tightlist
\item
  Explain strategy of using BES data and resulting dataset of 7503
  item-pairs
\item
  Descriptive analyses:

  \begin{itemize}
  \tightlist
  \item
    Define a measure for extent of constrainment
  \item
    Describe univariate distribution of this measure; asymmetry etc.
    -Probability of reaching the r-max and r-min (and implications: does
    this mean that the constrainment is even more severe?)
  \end{itemize}
\end{itemize}

Factors influencing the degree to which correlations are constrained

\begin{itemize}
\tightlist
\item
  Analyses of the same dataset of 7503 item pairs
\item
  Other characteristics of these include

  \begin{itemize}
  \tightlist
  \item
    Characteristics of each of the two vars separately (their variance /
    skew / kurtosis / mean etc), as well as dyadic characteristics
    (difference of means / skew/ etc)
  \item
    Hypotheses/expectations:

    \begin{itemize}
    \tightlist
    \item
      Effect of difference of means and of difference of skew
    \item
      Perhaps effect of number of categories (constrainment less severe
      the larger the number of categories)
    \end{itemize}
  \end{itemize}
\end{itemize}

Consequences of constrained correlations in applied research

\begin{itemize}
\tightlist
\item
  Descriptive interpretation of correlations (Cohen's criteria) is
  undermined
\item
  It is also undermined by every correlation being expressed on its own
  unique scale/unique calibration
\item
  Need a nice example of 2 correlations with \(r(a,b) \le r(b,c)\) and
  where \(r(a,b)\)max is \textless{} \(r(b.c)\): is it smaller because
  the linear relationship is weaker, or because he constrainment is
  stronger?
\end{itemize}

\section{Background and Definitions}\label{background-and-definitions}

\begin{itemize}
\tightlist
\item
  Ordinal data and assumptions of interval-scale correlation\\
\item
  Existing alternatives: Spearman, polychoric, polyserial; their
  strengths and limitations\\
\item
  Coupling and rearrangement in discrete settings
\end{itemize}

\section{Theoretical Framework}\label{theoretical-framework}

\subsection{Optimal Coupling and Correlation
Bounds}\label{optimal-coupling-and-correlation-bounds}

We formalize the problem of identifying the joint distribution
(coupling) of two ordinal variables with fixed marginals that maximizes
or minimizes the Pearson correlation.

\begin{proposition}


Let $X$ and $Y$ be discrete ordinal random variables, each with finite ordered support:

$$
X: x_1 < x_2 < \dots < x_{K_X},\quad Y: y_1 < y_2 < \dots < y_{K_Y}
$$

with marginal probability distributions $p_X(x_i) = P(X = x_i)$, $p_Y(y_j) = P(Y = y_j)$, respectively.

Then, the maximum and minimum values of the Pearson correlation coefficient $r_{XY}$,


consistent with the fixed marginals, are obtained as follows:

- Maximum $r_{XY}$: achieved by pairing largest $X$-values with largest $Y$-values (comonotonic arrangement).
    
- Minimum $r_{XY}$: achieved by pairing largest $X$-values with smallest $Y$-values (anti-comonotonic arrangement).
    
\end{proposition}
\begin{proof}



The Pearson correlation coefficient is given by:

$$r_{XY} = \frac{E[XY] - E[X]E[Y]}{\sigma_X \sigma_Y}.$$

Given fixed marginals $p_X(x_i)$, $p_Y(y_j)$, the expectations $E[X], E[Y]$ and standard deviations $\sigma_X, \sigma_Y$ are constant. Thus, to maximize or minimize $r_{XY}$, we only need to maximize or minimize the expectation $E[XY]$:

$$E[XY] = \sum_{i=1}^{K_X}\sum_{j=1}^{K_Y} x_i y_j P(X = x_i, Y = y_j).$$

This proof relies on a rearrangement inequality of [@hard52-inequalities] (Theorem 368): 

For real sequences $a_1 \leq a_2 \leq \dots \leq a_n$ and $b_1 \leq b_2 \leq \dots \leq b_n$, and for any permutation $\pi$ of indices $\{1,\dots,n\}$ we have the following:

    

$$a_1 b_1 + a_2 b_2 + \dots + a_n b_n \geq a_1 b_{\pi(1)} + a_2 b_{\pi(2)} + \dots + a_n b_{\pi(n)}$$
and
 
    
$$a_1 b_n + a_2 b_{n-1} + \dots + a_n b_1 \leq a_1 b_{\pi(1)} + a_2 b_{\pi(2)} + \dots + a_n b_{\pi(n)}$$

Equality occurs only if the permutation $\pi$ results in a  monotonically increasing sequence (for maximum) or monotonically decreasing sequence (for minimum).



To explicitly connect this to our problem, enumerate observations explicitly from sorted marginals:
    
        $$X_{\downarrow}: x_{[K_X]} \geq x_{[K_X-1]} \geq \dots \geq x_{[1]}, \quad  
        Y_{\downarrow}: y_{[K_Y]} \geq y_{[K_Y-1]} \geq \dots \geq y_{[1]}.$$
(Sorted in descending order for maximization)

- Expand discrete probability distributions into explicit observation-level sequences. 
If $K_X \neq K_Y$, extend the shorter sequence with zero-probability categories so both sequences have equal length, which does not affect marginal probabilities or correlation.

Hence, to maximize $E[XY]$: pair the largest ranks of $X$ with the largest ranks of $Y$. Explicitly, start from the top (highest values) and move downward:
    
- Pair as many observations of the largest $X$-category as possible with the largest $Y$-category, then next-largest, etc., until all observations are paired.
        
- By the Hardy–Littlewood–Pólya Rearrangement Inequality, this explicitly constructed pairing yields the maximum possible sum of products, hence maximizing $E[XY]$.
    

Likewise, to minimize $E[XY]$ pair the largest $X$-categories with the smallest $Y$-categories, then second-largest $X$-categories with second-smallest $Y$-categories, and so forth (anti-comonotonic ordering).   By the rearrangement inequality, this arrangement explicitly yields the minimal sum of products, hence minimizing $E[XY]$.
    


\end{proof}

In words, given fixed marginals, we have shown:

\begin{itemize}
\item
  \textbf{Maximum correlation} \(r_{\text{max}}\) is achieved by
  comonotonic (DEFINE THIS!) arrangement:

  \[P_{\text{max}}(X=x_{[i]}, Y=y_{[i]}) \quad\text{in either descending or ascending order.}\]
\item
  \textbf{Minimum correlation} \(r_{\text{min}}\) is achieved by
  anti-comonotonic arrangement:

  \[P_{\text{min}}(X=x_{[i]}, Y=y_{[n+1-i]}) \quad\text{pairing descending X with ascending Y, or viceversa.}\]
\end{itemize}

With this explicit construction we can simply calculate \(r_{max}\) and
\(r_{min}\) when the values of \(X\) and \(Y\) are ranks.

\begin{proposition}[Corollary: Analytic Forms for $r_{\min}$ and $r_{\max}$]


Let two ordinal variables $X$ and $Y$ have values as follows:

* $X$ with $K_X$ ordinal levels: $1,2,\dots,K_X$
    
* $Y$ with $K_Y$ ordinal levels: $1,2,\dots,K_Y$
    

with absolute frequencies:

- $n_{X}(i)$, number of observations at level $i$ of $X$.
    
- $n_{Y}(j)$, number of observations at level $j$ of $Y$.
    

Then the maximum Pearson correlation is:

$$r_{\text{max}} = \frac{E[XY]_{\text{max}} - \bar{X}\bar{Y}}{\sigma_X \sigma_Y}.$$
Where 

$$
E[XY]_{\max} =\frac{1}{N}\sum_{i=1}^{K_X}\sum_{j=1}^{K_Y}x_i y_j M_{i,j}
$$

where explicitly defined frequencies $M_{i,j}$ pair observations in a comonotonic manner, calculated explicitly via the greedy iterative algorithm as follows.  

Define the matrix $M_{i,j}$, the frequency with which the level $x_i$ pairs with the level $y_j$. Clearly, we must have:

$$\sum_j M_{i,j}=n_X(i),\quad\sum_i M_{i,j}=n_Y(j)$$

**Algorithm for explicit calculation of $M_{i,j}$** (greedy method):


Just a sample algorithmn
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Write here the result}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Marginal frequencies  $n_{X}(i), i \in \{1,2,\ldots, K_X  \}$ and  $n_{Y}(j) j \in \{1,2,\dots, K_Y  \}$}
\Output{A $K_X$ by $K_Y$ matrix, $M$}
\BlankLine
\While{While condition}{
    instructions\;
    \eIf{condition}{
        instructions1\;
        instructions2\;
    }{
        instructions3\;
    }
}
\caption{While loop with If/Else condition}


Initialize available frequencies explicitly:  $n'_Y(j)=n_Y(j)\quad\text{for each }j$

- For each level $x_i$, from smallest $i=1$ to largest:
    
    - For each level $y_j$, from smallest $j=1$ to largest:
        
        - Pair as many observations as possible:
            

$$M_{i,j}=\min\left(n_X(i)-\sum_{s=1}^{j-1}M_{i,s},\, n'_Y(j)\right)$$

Next, update explicitly: $$n'_Y(j)\leftarrow n'_Y(j)-M_{i,j}$$

\end{algorithm}


Similarly, the minimum Pearson correlation ($r_{\text{min}}$) achieved by anti-comonotonic alignment (pairing largest ranks of $X$ with smallest ranks of $Y$) is explicitly:
$$r_{\text{min}} = \frac{E[XY]_{\text{min}} - \bar{X}\bar{Y}}{\sigma_X \sigma_Y}.$$



\end{proposition}
\begin{proof}

Let the total number of observations be $N$, thus:

$$\sum_{i=1}^{K_X} n_{X}(i) = N,\quad \sum_{j=1}^{K_Y} n_{Y}(j) = N.$$

Means and standard deviations clearly become:

Means:
    
$$
\bar{X} = \frac{1}{N}\sum_{i=1}^{K_X} i \cdot n_X(i),\quad  
    \bar{Y} = \frac{1}{N}\sum_{j=1}^{K_Y} j \cdot n_Y(j).
$$
Standard deviations:
    
$$
\sigma_X = \sqrt{\frac{1}{N}\sum_{i=1}^{K_X} (i - \bar{X})^2 n_X(i)},\quad  
    \sigma_Y = \sqrt{\frac{1}{N}\sum_{j=1}^{K_Y} (j - \bar{Y})^2 n_Y(j)}.
$$


To achieve the maximum expected product $E[XY]_{\text{max}}$, do the following:
First *sort* ranks from largest to smallest independently for both $X$ and $Y$; then *pair* ranks directly from highest to lowest available observations.
    




**Minimum Expectation**


Define explicitly a matrix $R_{i,j}$ giving frequencies of pairings $X=x_i$ with $Y=y_j$. For the minimal expectation, pair largest available $X$-values explicitly with smallest available $Y$-values first (greedy algorithm):

Initialize explicitly available frequencies:
    

$$n'_Y(j)=n_Y(j)\quad\text{for each }j$$

- For $i$ from largest to smallest ($i=K_X$ down to $1$):
    
    - For $j$ from smallest to largest ($j=1$ up to $K_Y$):
        - Pair explicitly as many observations as possible:
$$R_{i,j}=\min\left(n_X(i)-\sum_{s=1}^{j-1}R_{i,s},\, n'_Y(j)\right)$$

- Update explicitly available frequencies:


$$n'_Y(j)\leftarrow n'_Y(j)-R_{i,j}$$

Then explicitly the minimal expectation is:

$$E[XY]_{\text{min}}=\frac{1}{N}\sum_{i=1}^{K_X}\sum_{j=1}^{K_Y} x_i y_j R_{i,j}$$


This explicit equation applies Whitt’s rearrangement theorem  (@whit76-bivariate), ensuring proper maximal pairing of ranks.

The formula for $E[XY]_{\text{min}}$ is derived in a similar maner.

\end{proof}

\textbf{Discussion}

We want to construct a joint distribution table \(M\) such that:

\begin{itemize}
\item
  The \textbf{row sums} match the marginal counts for \(X\):
  \(\sum_j M_{i,j} = n_X(i)\)
\item
  The \textbf{column sums} match the marginal counts for \(Y\):
  \(\sum_i M_{i,j} = n_Y(j)\)
\end{itemize}

To do this, we go \textbf{greedily}, pairing the highest available \(X\)
values with the \textbf{smallest available} \(Y\) values, one cell at a
time.

To avoid \textbf{exceeding} the available marginal totals in \(Y\), we
must track how much of \(n_Y(j)\) is \textbf{still unassigned} at each
step --- and that's what \(n_Y'(j)\) represents.\\
After assigning \(M_{i,j}\) units to the cell \((i,j)\), we subtract
that amount from the remaining pool of level \(y_j\) values, like this:
\(n_Y'(j) \leftarrow n_Y'(j) - M_{i,j}\)This ensures we don't
\textbf{overfill} any column of the matrix.

To maximize the expectation \(E[XY]\), the rearrangement inequality
states we must pair values of \(X\) and \(Y\) from smallest-to-smallest,
second-smallest-to-second-smallest, and so forth (comonotonic
alignment).

Thus, explicitly:

\begin{itemize}
\tightlist
\item
  Sort \(X\): lowest-to-highest:
\end{itemize}

\[X_{\text{sorted}} = (\underbrace{x_1,\dots,x_1}_{n_X(1)}, \underbrace{x_2,\dots,x_2}_{n_X(2)}, \dots,\underbrace{x_{K_X},\dots,x_{K_X}}_{n_X(K_X)})\]

Sorted \(Y\)-values (ascending order for max, descending for min):

\[Y_{\text{sorted(max)}} = (\underbrace{y_1,\dots,y_1}_{n_Y(1)}, \underbrace{y_2,\dots,y_2}_{n_Y(2)}, \dots,\underbrace{y_{K_Y},\dots,y_{K_Y}}_{n_Y(K_Y)})\]

\[Y_{\text{sorted(min)}} = (\underbrace{y_{K_Y},\dots,y_{K_Y}}_{n_Y(K_Y)}, \underbrace{y_{K_Y-1},\dots,y_{K_Y-1}}_{n_Y(K_Y-1)}, \dots,\underbrace{y_1,\dots,y_1}_{n_Y(1)})\]

The maximum and minimum expectations are simply:

\textbf{Maximum expectation} (pair element-by-element):

\[E[XY]_{\text{max}} = \frac{1}{N}\sum_{k=1}^{N} X_{\text{sorted}}(k)\cdot Y_{\text{sorted(max)}}(k)\]

\begin{itemize}
\tightlist
\item
  \textbf{Minimum expectation}:
\end{itemize}

\[E[XY]_{\text{min}} = \frac{1}{N}\sum_{k=1}^{N} X_{\text{sorted}}(k)\cdot Y_{\text{sorted(min)}}(k)\]

These classical bounds define feasible joint distributions given
marginals and constrain the attainable values of \(r\). There is an
interesting conncetion to Fréchet--Hoeffding and Boole--Fréchet
Inequalities, which we note but leave for future research.\footnote{Fréchet--Hoeffding
  and Boole--Fréchet Inequalities

  This is the second paragraph. Indent each new paragraph with four
  spaces or a tab, just like you would for a multi-paragraph list item.

  You can also include code blocks:

\begin{Verbatim}
print("Hello, footnote!")
\end{Verbatim}

  Or even more paragraphs and content as needed.}

\subsection{Symmetry and the Role of
Ties}\label{symmetry-and-the-role-of-ties}

\begin{itemize}
\tightlist
\item
  Conditions under which \(r_{\min} = -r_{\max}\)
\item
  How ties in marginal distributions affect the attainability of the
  lower bound
\end{itemize}

\subsection{Special Cases and Boundary
Conditions}\label{special-cases-and-boundary-conditions}

\begin{itemize}
\tightlist
\item
  Equal vs.~unequal numbers of categories\\
\item
  Symmetric vs.~asymmetric marginal distributions
\end{itemize}

\subsection{Relation to Copula Theory
(Optional)}\label{relation-to-copula-theory-optional}

Although our setting is discrete, the problem is conceptually linked to
copula-based modeling of dependence with fixed marginals. The connection
is discussed as a direction for future work.

\section{Empirical Illustration and Practical
Relevance}\label{empirical-illustration-and-practical-relevance}

\subsection{Effects of Marginal
Shapes}\label{effects-of-marginal-shapes}

\begin{itemize}
\tightlist
\item
  Shape-based elasticity of correlation bounds\\
\item
  Influence of skewness, modality, and entropy
\end{itemize}

\subsection{Symmetry Breaking in Correlation
Bounds}\label{symmetry-breaking-in-correlation-bounds}

\begin{itemize}
\tightlist
\item
  When and why \(r_{\min} \neq -r_{\max}\)
\item
  Empirical indicators of bound asymmetry
\end{itemize}

When we talk about ``asymmetry'' in this context, we're referring to how
different the distribution of Y is from the distribution of X. Since X
is kept uniform, any deviation of Y from uniformity creates asymmetry
between the distributions. The \emph{total variation distance} (TV)
quantifies this asymmetry:

\begin{itemize}
\tightlist
\item
  It measures how different Y is from being symmetric around its
  midpoint
\item
  When TV = 0: \(Y\) is symmetric (like \(X\))
\item
  When TV is large: \(Y\) is highly asymmetric compared to \(X\)
\end{itemize}

\subsection{Applications and
Implications}\label{applications-and-implications}

\begin{itemize}
\tightlist
\item
  Impact on PCA, factor analysis, and SEM using ordinal data\\
\item
  Practical risks: over-factoring, biased structural coefficients\\
\item
  Recommendations for interpreting Pearson's \(r\) in applied contexts
\end{itemize}

\section{Extensions and Open
Questions}\label{extensions-and-open-questions}

\begin{itemize}
\tightlist
\item
  Role of entropy and joint uncertainty in bounding behavior\\
\item
  Probabilistic modeling over the \(r \in [r_{\min}, r_{\max}]\)
  interval\\
\item
  Possibility of adjusting or normalizing \(r\)
\end{itemize}

\subsection{To Do}\label{to-do}

\begin{itemize}
\tightlist
\item
  Can \(r_{min} >0\)?
\item
  Are there systematic biases in testing \(H_0: r=0\) using t-test?
  (Answer with Monte Carlo simulated data?)
\item
  Add: ``\(r_min\) is invariant to `problem 1' (value assignment. So
  using ranks is actually w.l.o.g.)
\end{itemize}

\section{Conclusion}\label{conclusion}

\begin{itemize}
\tightlist
\item
  Summary: Theoretical bounds, symmetry-breaking conditions, and
  implications for applied research
\item
  An R package accompanies this paper, providing tools to compute
  max/min bounds and perform randomization-based hypothesis tests for
  fixed marginals. Details are available in the appendix and online.
\item
  Future work: deeper integration with copula models, correction
  strategies, and generalizations to higher dimensions
\end{itemize}

\newpage

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-whit76-bivariate}
Whitt, W. (1976). Bivariate {Distributions} with {Given Marginals}.
\emph{The Annals of Statistics}, \emph{4}(6), 1280--1289.

\end{CSLReferences}

\newpage

\appendix

\subsection{Appendix A: Mathematical
Proofs}\label{appendix-a-mathematical-proofs}

\begin{itemize}
\tightlist
\item
  Formal derivations of optimal coupling and analytical bounds
\end{itemize}

\subsection{Appendix B: Software Tools and Code
Listings}\label{appendix-b-software-tools-and-code-listings}

\begin{itemize}
\tightlist
\item
  Documentation and examples for the R package\\
\item
  Description of randomization testing features
\end{itemize}

\subsection{Appendix C: Additional Tables and
Simulations}\label{appendix-c-additional-tables-and-simulations}

\begin{itemize}
\tightlist
\item
  Supplementary empirical examples, simulation results
\end{itemize}




\end{document}
