% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{mathpazo}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\usepackage{float}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage[style=apa,sorting=nyt]{biblatex}
\addbibresource{references.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Bounding Pearson's Correlation for Ordinal Data: Theory, Symmetry, and Empirical Consequences},
  pdfauthor={Cees van der Eijk; Scott Moser},
  colorlinks=true,
  linkcolor={black},
  filecolor={Maroon},
  citecolor={RoyalBlue},
  urlcolor={BrickRed},
  pdfcreator={LaTeX via pandoc}}


\title{Bounding Pearson's Correlation for Ordinal Data: Theory,
Symmetry, and Empirical Consequences}
\author{Cees van der Eijk \and Scott Moser}
\date{}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\subsection{3. Maximizing and Minimizing Pearson's
Correlation}\label{maximizing-and-minimizing-pearsons-correlation}

To determine the maximum and minimum possible Pearson's correlation
coefficients given the marginal distributions, we employ the concept of
optimal coupling derived from the Hardy--Littlewood--Pólya rearrangement
inequality.

\subsubsection{3.1 Maximizing Pearson's
Correlation}\label{maximizing-pearsons-correlation}

To maximize Pearson's correlation, we construct a joint distribution
that pairs categories from \$X\$ and \$Y\$ in a comonotonic (similarly
ordered) fashion. Specifically, the optimal joint distribution, denoted
as \$F\_\{\text{max}\}(x,y)\$, is constructed by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sorting the categories of \$X\$ in descending order according to their
  values.
\item
  Independently sorting the categories of \$Y\$ in descending order
  according to their values.
\item
  Pairing the largest values of \$X\$ with the largest values of \$Y\$,
  the next-largest with the next-largest, and so forth, maintaining
  consistency with the marginal totals.
\end{enumerate}

Formally, if \$x\_\{(1)\} \geq x\_\{(2)\}
\geq \dots \geq x\_\{(K\_X)\}\$ and \$y\_\{(1)\} \geq y\_\{(2)\}
\geq \dots \geq y\_\{(K\_Y)\}\$ represent the sorted categories of \$X\$
and \$Y\$, respectively, then the joint probabilities are allocated to
maximize:

\[
\mathbb{E}[XY]_{\text{max}} = \sum_{i,j} x_{(i)} y_{(j)} P_{\text{max}}(X = x_{(i)}, Y = y_{(j)}).
\]

\subsubsection{3.2 Minimizing Pearson's
Correlation}\label{minimizing-pearsons-correlation}

Conversely, to minimize Pearson's correlation, we construct the joint
distribution \$F\_\{\text{min}\}(x,y)\$ by pairing categories of \$X\$
and \$Y\$ in a countermonotonic (oppositely ordered) fashion.
Specifically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sorting the categories of \$X\$ in descending order according to their
  values.
\item
  Independently sorting the categories of \$Y\$ in ascending order
  according to their values.
\item
  Pairing the largest values of \$X\$ with the smallest values of \$Y\$,
  and so forth, while respecting marginal totals.
\end{enumerate}

The minimum expected value under this joint distribution is:

\[
\mathbb{E}[XY]_{\text{min}} = \sum_{i,j} x_{
\]

\subsection{3. Maximizing and Minimizing Pearson's
Correlation}\label{maximizing-and-minimizing-pearsons-correlation-1}

To determine the maximum and minimum possible Pearson's correlation
coefficients given the marginal distributions, we employ the concept of
optimal coupling derived from the Hardy--Littlewood--Pólya rearrangement
inequality.

\subsubsection{3.1 Maximizing Pearson's
Correlation}\label{maximizing-pearsons-correlation-1}

To maximize Pearson's correlation, we construct a joint distribution
that pairs categories from \$X\$ and \$Y\$ in a comonotonic (similarly
ordered) fashion. Specifically, the optimal joint distribution, denoted
as \$F\_\{\text{max}\}(x,y)\$, is constructed by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sorting the categories of \$X\$ in descending order according to their
  values.
\item
  Independently sorting the categories of \$Y\$ in descending order
  according to their values.
\item
  Pairing the largest values of \$X\$ with the largest values of \$Y\$,
  the next-largest with the next-largest, and so forth, maintaining
  consistency with the marginal totals.
\end{enumerate}

Formally, if \$x\_\{(1)\} \geq x\_\{(2)\}
\geq \dots \geq x\_\{(K\_X)\}\$ and \$y\_\{(1)\} \geq y\_\{(2)\}
\geq \dots \geq y\_\{(K\_Y)\}\$ represent the sorted categories of \$X\$
and \$Y\$, respectively, then the joint probabilities are allocated to
maximize:

\[
\mathbb{E}[XY]_{\text{max}} = \sum_{i,j} x_{(i)} y_{(j)} P_{\text{max}}(X = x_{(i)}, Y = y_{(j)}).
\]

\subsubsection{3.2 Minimizing Pearson's
Correlation}\label{minimizing-pearsons-correlation-1}

Conversely, to minimize Pearson's correlation, we construct the joint
distribution \$F\_\{\text{min}\}(x,y)\$ by pairing categories of \$X\$
and \$Y\$ in a countermonotonic (oppositely ordered) fashion.
Specifically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sorting the categories of \$X\$ in descending order according to their
  values.
\item
  Independently sorting the categories of \$Y\$ in ascending order
  according to their values.
\item
  Pairing the largest values of \$X\$ with the smallest values of \$Y\$,
  and so forth, while respecting marginal totals.
\end{enumerate}

The minimum expected value under this joint distribution is:

\[
\mathbb{E}[XY]_{\text{min}} = \sum_{i,j} x_{(i)} y_{(K_Y - j + 1)} P_{\text{min}}(X = x_{(i)}, Y = y_{(K_Y - j + 1)}).
\]

\subsubsection{3.3 Computing Pearson's Correlation
Bounds}\label{computing-pearsons-correlation-bounds}

Given the optimal pairings, the maximum and minimum Pearson's
correlations are calculated as:

\[
r_{\text{max}} = \frac{\mathbb{E}[XY]_{\text{max}} - \mathbb{E}[X]\mathbb{E}[Y]}{\sqrt{\text{Var}(X)\text{Var}(Y)}}, \quad r_{\text{min}} = \frac{\mathbb{E}[XY]_{\text{min}} - \mathbb{E}[X]\mathbb{E}[Y]}{\sqrt{\text{Var}(X)\text{Var}(Y)}}.
\]

These bounds precisely characterize the feasible range of correlation
coefficients given the fixed marginal distributions, providing a
rigorous basis for interpreting correlations derived from ordinal data.

\subsection{4. Connection to Fréchet--Hoeffding
Bounds}\label{connection-to-fruxe9chethoeffding-bounds}

The theoretical bounds on Pearson's correlation given fixed marginals
are closely related to the classical Fréchet--Hoeffding bounds. These
bounds define the feasible region for joint distributions of two random
variables given their marginal distributions.

\subsubsection{4.1 Fréchet--Hoeffding
Bounds}\label{fruxe9chethoeffding-bounds}

The Fréchet--Hoeffding bounds specify the limits of joint cumulative
distribution functions (CDFs) given marginals \$F\_X(x)\$ and
\$F\_Y(y)\$:

\[
\max\{F_X(x) + F_Y(y) - 1, 0\} \leq F_{XY}(x,y) \leq \min\{F_X(x), F_Y(y)\}.
\]

These bounds represent the minimum and maximum possible joint
probabilities for each pair of categories \$(x,y)\$.

\subsubsection{4.2 Achievability of
Bounds}\label{achievability-of-bounds}

The maximum Pearson correlation, as described in Section 3, is achieved
precisely when the joint distribution aligns with the upper
Fréchet--Hoeffding bound (comonotonic arrangement). Conversely, the
minimum correlation corresponds to the lower Fréchet--Hoeffding bound
(countermonotonic arrangement).

\begin{itemize}
\tightlist
\item
  \textbf{Upper Bound (comonotonic):} Always achievable as probabilities
  can be consistently allocated by aligning cumulative marginals.
\item
  \textbf{Lower Bound (countermonotonic):} Often not achievable in
  practice due to structural constraints like ties or discrete
  probability masses, making it impossible to perfectly align cumulative
  marginals inversely.
\end{itemize}

\subsubsection{4.3 Role of Asymmetry and
Ties}\label{role-of-asymmetry-and-ties}

Asymmetry in the marginal distributions or the presence of ties
significantly impacts the achievability and symmetry of these bounds:

\begin{itemize}
\tightlist
\item
  Marginal distributions with equal or symmetrical frequencies typically
  yield symmetric correlation bounds (i.e., \$r\_\{\text{min}\}
  \approx -r\_\{\text{max}\}\$).
\item
  Marginal distributions with strong asymmetry or significant ties
  typically yield asymmetric bounds (\$r\_\{\text{min}\}
  \neq -r\_\{\text{max}\}\$), reflecting structural constraints in the
  joint probability allocation.
\end{itemize}

The asymmetry phenomenon underscores the importance of considering
distribution shape and entropy in interpreting correlation results.

\subsubsection{4.4 Practical Implications}\label{practical-implications}

Recognizing these connections provides crucial insights for applied
researchers:

\begin{itemize}
\tightlist
\item
  \textbf{Interpretation:} Observed correlation coefficients should be
  contextualized within their theoretically permissible range.
\item
  \textbf{Diagnostics:} Calculation of Fréchet--Hoeffding bounds offers
  a diagnostic tool for evaluating whether reported correlations in
  empirical research are substantively meaningful or artifacts of
  marginal constraints.
\end{itemize}

In subsequent sections, we explore specific conditions under which
asymmetry arises, providing explicit analytical conditions and empirical
illustrations to guide practical interpretation.

\subsection{5. Symmetry and Asymmetry in Correlation
Bounds}\label{symmetry-and-asymmetry-in-correlation-bounds}

A critical insight from the analysis of correlation bounds is that the
achievable maximum and minimum Pearson correlations are not always
symmetric around zero. This section explores conditions under which this
symmetry holds or breaks and introduces entropy as a measure of marginal
asymmetry.

\subsubsection{5.1 Defining Symmetry in Correlation
Bounds}\label{defining-symmetry-in-correlation-bounds}

Symmetry in correlation bounds occurs when:

\[
r_{\text{min}} = -r_{\text{max}}
\]

This equality implies that the magnitudes of the highest achievable
positive and negative correlations are identical.

\subsubsection{5.2 Conditions for
Symmetry}\label{conditions-for-symmetry}

Symmetry typically arises under conditions of balanced or symmetric
marginal distributions. More explicitly:

\begin{itemize}
\tightlist
\item
  Marginal distributions with uniform frequencies or probabilities.
\item
  Marginal distributions symmetric about their median category.
\end{itemize}

Under these conditions, the cumulative probability masses can be aligned
or inversely aligned without structural discrepancies, maintaining
symmetric correlation bounds.

\subsubsection{5.3 Conditions for
Asymmetry}\label{conditions-for-asymmetry}

Asymmetry (\$r\_\{\text{min}\} \ne -r\_\{\text{max}\}\$) commonly arises
under the following scenarios:

\begin{itemize}
\tightlist
\item
  Strongly skewed marginal distributions: When one or both marginal
  distributions are heavily skewed, the allocation of joint
  probabilities inherently favors one direction of correlation over the
  other.
\item
  Presence of ties: Significant ties in marginal distributions limit the
  flexibility of inverse alignment in cumulative distributions, causing
  structural constraints that prevent the achievement of perfectly
  inverse pairings.
\end{itemize}

\subsubsection{5.4 Visualizing Symmetry and
Asymmetry}\label{visualizing-symmetry-and-asymmetry}

Visualization strategies, such as heatmaps and entropy-based diagrams,
illustrate how marginal distributions transition from symmetric to
asymmetric and the corresponding impact on achievable correlation
bounds. Such visualizations aid researchers in understanding and
interpreting empirical correlation results within their permissible
theoretical ranges.

\subsubsection{5.5 Role of Entropy in Measuring
Asymmetry}\label{role-of-entropy-in-measuring-asymmetry}

Entropy provides a useful measure of distributional symmetry or
asymmetry. Higher entropy typically indicates a more uniform
(symmetrical) distribution, while lower entropy signals concentrated
(asymmetrical) probabilities. Formally, entropy (\$H\$) is given by:

\[
H(X) = -\sum_{i=1}^{K_X} p_X(x_i) \log p_X(x_i), \quad H(Y) = -\sum_{j=1}^{K_Y} p_Y(y_j) \log p_Y(y_j).
\]

As entropy decreases, indicating greater skewness or concentration in
probability masses, the bounds on correlations typically become more
asymmetric.

\subsubsection{5.6 Empirical and Practical
Considerations}\label{empirical-and-practical-considerations}

Empirically, assessing symmetry or asymmetry in correlation bounds
provides researchers with important context for interpreting results:

\begin{itemize}
\tightlist
\item
  Symmetric bounds imply similar interpretative weight for positive and
  negative correlations.
\item
  Asymmetric bounds highlight the necessity for cautious interpretation,
  especially regarding the relative magnitude and substantive
  significance of observed correlations.
\end{itemize}

Recognizing symmetry conditions and their practical implications thus
plays an essential role in robustly interpreting correlations in
empirical analyses of ordinal data.

\newpage

\section{References}\label{references}

\printbibliography[heading=none]

\newpage





\end{document}
